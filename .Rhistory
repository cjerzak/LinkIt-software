rep(1,times = T1))
DMat = matrix(0,nrow = N,ncol=T0+T1)
DMat[G_obs==1,] <-t(replicate(c(PreIndicator), n = sum(G_obs==1)))
head(G_obs)
head(DMat)
#Notice the difference!
#Now, we can generate the observed outcomes.
outcome_coefs <- rnorm(k)
YPeriod0 <- X %*% outcome_coefs + rnorm(N, sd = 0.20)
YPeriod1 <- YPeriod0 + tau * G_obs + rnorm(N, sd = 0.50 * abs(YPeriod0)) #what is this scaling in the sd doing?
#Let's move on to the estimation!
#First, we need to estimate the propensity scores
#We  do so using lasso-regularized multionomial regression model
#regularization penalty chosen via 10-fold cross-validation
library(glmnet)
my_lasso = cv.glmnet(x = X,
y = as.matrix(G_obs),
family = "binomial",
type.measure = "class",
nfolds = 10,
alpha = 1)
#plot( my_lasso )
#Grab predicted probabilities
#s = lambda.min means that we want the predicted probabilities
#when the regularization penalty minimizes the mis-classification error
propensityEst = c( predict(my_lasso, s = "lambda.min", newx = X))
tauhat_vec[i] = mean( c(YPeriod1 - YPeriod0) * (G_obs - propensityEst) /
(propensityEst * (1-propensityEst)) )
}
#store bias and variance given DGP
biasVec[counter_i] <- mean(tauhat_vec - tau)
varVec[counter_i] <- var(tauhat_vec)
}
#Great! We're ready to plot our results
#Would you expect things to behave in a linear way or
#non-linear way with respect to the fraction of
#covariates incorrectly specified?
plot(seq_CovariateIncorrectlySpecified/k, abs(biasVec),
xlab = "fraction of covariates incorrectly specified",
ylab = "Absolute Bias")
plot(seq_CovariateIncorrectlySpecified/k, varVec,
xlab = "fraction of covariates incorrectly specified",
ylab = "Variance")
plot(seq_CovariateIncorrectlySpecified/k, varVec+biasVec^2,
xlab = "fraction of covariates incorrectly specified",
ylab = "Mean-squared Error (MSE)")
#!! Answer: Non-linear relationships due to fact that tau-hat is not
#a linear function of the estimated propensities.
#We're using the wrong model and the way in which errors in
#model specification propogate is non-linear (rather hard to capture analytically).
#Notice that the problem isn't only due to propensities going near to 0 or near to 1
#To see this, just check this out (the estimated propensities from the last iteration)
hist( propensityEst )
#The estimated propensities are actually centered around 0.5 here.
#The true propensities are closer to 0 and 1 (but even this is not a severe problem here)
hist( propensityTrue )
#Great! We're ready to plot our results
#Would you expect things to behave in a linear way or
#non-linear way with respect to the fraction of
#covariates incorrectly specified?
plot(seq_CovariateIncorrectlySpecified/k, abs(biasVec),
xlab = "fraction of covariates incorrectly specified",
ylab = "Absolute Bias")
plot(seq_CovariateIncorrectlySpecified/k, varVec,
xlab = "fraction of covariates incorrectly specified",
ylab = "Variance")
plot(seq_CovariateIncorrectlySpecified/k, varVec+biasVec^2,
xlab = "fraction of covariates incorrectly specified",
ylab = "Mean-squared Error (MSE)")
#Great! We're ready to plot our results
#Would you expect things to behave in a linear way or
#non-linear way with respect to the fraction of
#covariates incorrectly specified?
plot(seq_CovariateIncorrectlySpecified/k, abs(biasVec),
xlab = "fraction of covariates incorrectly specified",
ylab = "Absolute Bias")
#GOAL 1. To illustrate the inverse propensity weighting method
#       by using Q6 in the homework.
#GOAL 2. Clarify the G_i, P_t, D_i notation using inverse propensity weighting
#In this study, we're going to study the setup in Q6 in the homework.
#In particular, we're going to check out how the bias + variance of the inverse-propensity weighting (IPW) estimator
#behaves under varying degrees of model misspecification!
#clear workspace
rm(list=ls())
library(glmnet) #load in glmnet package for lasso-penalized multinomial logistic reg.
set.seed(1)
T0 <- 1#1 pre-intervention period
T1 <- 1#1 post-intervention period
N = 1000 #total number of units in study
k = 20#how many covariates?
FracEverTreat = 0.5#fraction of units treated
tau = 0.25
seq_CovariateIncorrectlySpecified = round(c(0,k*0.25,k*0.5,k*0.75,k))
biasVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
varVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
counter_i <- 0
seq_CovariateIncorrectlySpecified
counter_i <- 0
seq_CovariateIncorrectlySpecified = round(c(0,k*0.25,k*0.5,k*0.75,k))
biasVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
varVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
counter_i <- 0
for(NumbCovarNonlinear in seq_CovariateIncorrectlySpecified){
print(sprintf("Getting results when %s of %s covariates incorrectly modeled",
NumbCovarNonlinear, k))
counter_i = counter_i + 1
#setup placeholder for inner results
#we need to average over many draws of the model
#with a fixed number of non-linear covariate relations
tauhat_vec = rep(NA,times=50)
for(i in 1:50){
#generate background covariates
X = mvtnorm::rmvnorm(n = N,
mean = rep(0,k),
sigma = diag(k)) #sample covariates -- what do you NOT notice here?
#these are the linear coefficients in the propensity model for G_i.
#we are going to manipulate the degree of non-linearity in X_i
#and see how our linear propensity score model (estimated via logistic regression)
#does at estimating the ATE.
propensity_coefs = rnorm(k)
if(NumbCovarNonlinear == 0){ logitTrue = scale(X %*% propensity_coefs + rnorm(N, sd = 0.25))}
if(NumbCovarNonlinear > 0){
X_transform = sapply(1:NumbCovarNonlinear,
function(i){
x = X[,i]
nonlinear_numb = sample(1:5,1)
if(nonlinear_numb == 1){x = sin(x)}
if(nonlinear_numb == 2){x = (x)^2}
if(nonlinear_numb == 3){x = log(abs(x))}
if(nonlinear_numb == 4){x = 1/(1+exp(-x))}
if(nonlinear_numb == 5){x = abs(x)}
return( x )
} )
X_tilde = cbind(X_transform,X[,-c(1:NumbCovarNonlinear)])
logitTrue = scale( X_tilde %*% propensity_coefs + rnorm(N, sd = 0.25) )
}
propensityTrue = 1 / (1 + exp(- logitTrue ))
#Remember, G_i denotes the ever-treated status of i, NOT its treatment status
G_obs = rbinom(N, size = 1, prob = propensityTrue)
#We could use G_i to construct D_{it}.
#Notice how G_i is indexed only by i. D_{it} is indexed by both i and t
PreIndicator = c(rep(0,times = T0),
rep(1,times = T1))
DMat = matrix(0,nrow = N,ncol=T0+T1)
DMat[G_obs==1,] <-t(replicate(c(PreIndicator), n = sum(G_obs==1)))
head(G_obs)
head(DMat)
#Notice the difference!
#Now, we can generate the observed outcomes.
outcome_coefs <- rnorm(k)
YPeriod0 <- X %*% outcome_coefs + rnorm(N, sd = 0.20)
YPeriod1 <- YPeriod0 + tau * G_obs + rnorm(N, sd = 0.50 * abs(YPeriod0)) #what is this scaling in the sd doing?
#Let's move on to the estimation!
#First, we need to estimate the propensity scores
#We  do so using lasso-regularized multionomial regression model
#regularization penalty chosen via 10-fold cross-validation
library(glmnet)
my_lasso = cv.glmnet(x = X,
y = as.matrix(G_obs),
family = "binomial",
type.measure = "class",
nfolds = 10,
alpha = 1)
#plot( my_lasso )
#Grab predicted probabilities
#s = lambda.min means that we want the predicted probabilities
#when the regularization penalty minimizes the mis-classification error
propensityEst = c( predict(my_lasso, s = "lambda.min", newx = X))
tauhat_vec[i] = mean( c(YPeriod1 - YPeriod0) * (G_obs - propensityEst) /
(propensityEst * (1-propensityEst)) )
}
#store bias and variance given DGP
biasVec[counter_i] <- mean(tauhat_vec - tau)
varVec[counter_i] <- var(tauhat_vec)
}
#Great! We're ready to plot our results
#Would you expect things to behave in a linear way or
#non-linear way with respect to the fraction of
#covariates incorrectly specified?
plot(seq_CovariateIncorrectlySpecified/k, abs(biasVec),
xlab = "fraction of covariates incorrectly specified",
ylab = "Absolute Bias")
plot(seq_CovariateIncorrectlySpecified/k, varVec,
xlab = "fraction of covariates incorrectly specified",
ylab = "Variance")
plot(seq_CovariateIncorrectlySpecified/k, varVec+biasVec^2,
xlab = "fraction of covariates incorrectly specified",
ylab = "Mean-squared Error (MSE)")
#!! Answer: Non-linear relationships due to fact that tau-hat is not
#a linear function of the estimated propensities.
#We're using the wrong model and the way in which errors in
#model specification propogate is non-linear (rather hard to capture analytically).
#Notice that the problem isn't only due to propensities going near to 0 or near to 1
#To see this, just check this out (the estimated propensities from the last iteration)
hist( propensityEst )
#Great! We're ready to plot our results
#Would you expect things to behave in a linear way or
#non-linear way with respect to the fraction of
#covariates incorrectly specified?
plot(seq_CovariateIncorrectlySpecified/k, abs(biasVec),
xlab = "fraction of covariates incorrectly specified",
ylab = "Absolute Bias")
plot(seq_CovariateIncorrectlySpecified/k, varVec,
xlab = "fraction of covariates incorrectly specified",
ylab = "Variance")
seq_CovariateIncorrectlySpecified
#GOAL 1. To illustrate the inverse propensity weighting method
#       by using Q6 in the homework.
#GOAL 2. Clarify the G_i, P_t, D_i notation using inverse propensity weighting
#In this study, we're going to study the setup in Q6 in the homework.
#In particular, we're going to check out how the bias + variance of the inverse-propensity weighting (IPW) estimator
#behaves under varying degrees of model misspecification!
#clear workspace
rm(list=ls())
library(glmnet) #load in glmnet package for lasso-penalized multinomial logistic reg.
set.seed(2)
T0 <- 1#1 pre-intervention period
T1 <- 1#1 post-intervention period
N = 1000 #total number of units in study
k = 20#how many covariates?
FracEverTreat = 0.5#fraction of units treated
tau = 0.25
seq_CovariateIncorrectlySpecified = round(c(0,k*0.25,k*0.5,k*0.75,k))
biasVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
varVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
counter_i <- 0
for(NumbCovarNonlinear in seq_CovariateIncorrectlySpecified){
print(sprintf("Getting results when %s of %s covariates incorrectly modeled",
NumbCovarNonlinear, k))
counter_i = counter_i + 1
#setup placeholder for inner results
#we need to average over many draws of the model
#with a fixed number of non-linear covariate relations
tauhat_vec = rep(NA,times=100)
for(i in 1:100){
#generate background covariates
X = mvtnorm::rmvnorm(n = N,
mean = rep(0,k),
sigma = diag(k)) #sample covariates -- what do you NOT notice here?
#these are the linear coefficients in the propensity model for G_i.
#we are going to manipulate the degree of non-linearity in X_i
#and see how our linear propensity score model (estimated via logistic regression)
#does at estimating the ATE.
propensity_coefs = rnorm(k)
if(NumbCovarNonlinear == 0){ logitTrue = scale(X %*% propensity_coefs + rnorm(N, sd = 0.25))}
if(NumbCovarNonlinear > 0){
X_transform = sapply(1:NumbCovarNonlinear,
function(i){
x = X[,i]
nonlinear_numb = sample(1:5,1)
if(nonlinear_numb == 1){x = sin(x)}
if(nonlinear_numb == 2){x = (x)^2}
if(nonlinear_numb == 3){x = log(abs(x))}
if(nonlinear_numb == 4){x = 1/(1+exp(-x))}
if(nonlinear_numb == 5){x = abs(x)}
return( x )
} )
X_tilde = cbind(X_transform,X[,-c(1:NumbCovarNonlinear)])
logitTrue = scale( X_tilde %*% propensity_coefs + rnorm(N, sd = 0.25) )
}
propensityTrue = 1 / (1 + exp(- logitTrue ))
#Remember, G_i denotes the ever-treated status of i, NOT its treatment status
G_obs = rbinom(N, size = 1, prob = propensityTrue)
#We could use G_i to construct D_{it}.
#Notice how G_i is indexed only by i. D_{it} is indexed by both i and t
PreIndicator = c(rep(0,times = T0),
rep(1,times = T1))
DMat = matrix(0,nrow = N,ncol=T0+T1)
DMat[G_obs==1,] <-t(replicate(c(PreIndicator), n = sum(G_obs==1)))
head(G_obs)
head(DMat)
#Notice the difference!
#Now, we can generate the observed outcomes.
outcome_coefs <- rnorm(k)
YPeriod0 <- X %*% outcome_coefs + rnorm(N, sd = 0.20)
YPeriod1 <- YPeriod0 + tau * G_obs + rnorm(N, sd = 0.50 * abs(YPeriod0)) #what is this scaling in the sd doing?
#Let's move on to the estimation!
#First, we need to estimate the propensity scores
#We  do so using lasso-regularized multionomial regression model
#regularization penalty chosen via 10-fold cross-validation
library(glmnet)
my_lasso = cv.glmnet(x = X,
y = as.matrix(G_obs),
family = "binomial",
type.measure = "class",
nfolds = 10,
alpha = 1)
#plot( my_lasso )
#Grab predicted probabilities
#s = lambda.min means that we want the predicted probabilities
#when the regularization penalty minimizes the mis-classification error
propensityEst = c( predict(my_lasso, s = "lambda.min", newx = X))
tauhat_vec[i] = mean( c(YPeriod1 - YPeriod0) * (G_obs - propensityEst) /
(propensityEst * (1-propensityEst)) )
}
#store bias and variance given DGP
biasVec[counter_i] <- mean(tauhat_vec - tau)
varVec[counter_i] <- var(tauhat_vec)
}
#Great! We're ready to plot our results
#Would you expect things to behave in a linear way or
#non-linear way with respect to the fraction of
#covariates incorrectly specified?
plot(seq_CovariateIncorrectlySpecified/k, abs(biasVec),
xlab = "fraction of covariates incorrectly specified",
ylab = "Absolute Bias")
plot(seq_CovariateIncorrectlySpecified/k, varVec,
xlab = "fraction of covariates incorrectly specified",
ylab = "Variance")
plot(seq_CovariateIncorrectlySpecified/k, varVec+biasVec^2,
xlab = "fraction of covariates incorrectly specified",
ylab = "Mean-squared Error (MSE)")
#!! Answer: Non-linear relationships due to fact that tau-hat is not
#a linear function of the estimated propensities.
#We're using the wrong model and the way in which errors in
#model specification propogate is non-linear (rather hard to capture analytically).
#Notice that the problem isn't only due to propensities going near to 0 or near to 1
#To see this, just check this out (the estimated propensities from the last iteration)
hist( propensityEst )
#The estimated propensities are actually centered around 0.5 here.
#The true propensities are closer to 0 and 1 (but even this is not a severe problem here)
hist( propensityTrue )
rm(list=ls())
library(glmnet) #load in glmnet package for lasso-penalized multinomial logistic reg.
set.seed(2)
T0 <- 1#1 pre-intervention period
T1 <- 1#1 post-intervention period
N = 1000 #total number of units in study
k = 20#how many covariates?
FracEverTreat = 0.5#fraction of units treated
tau = 0.25
N = 1000 #total number of units in study
seq_CovariateIncorrectlySpecified = round(c(0,k*0.25,k*0.5,k*0.75,k))
seq_CovariateIncorrectlySpecified
biasVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
varVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
counter_i <- 0
seq_CovariateIncorrectlySpecified
print(sprintf("Getting results when %s of %s covariates incorrectly modeled",
NumbCovarNonlinear, k))
counter_i = counter_i + 1
#setup placeholder for inner results
#we need to average over many draws of the model
#with a fixed number of non-linear covariate relations
tauhat_vec = rep(NA,times=100)
#generate background covariates
X = mvtnorm::rmvnorm(n = N,
mean = rep(0,k),
sigma = diag(k)) #sample covariates -- what do you NOT notice here?
dim(seq_CovariateIncorrectlySpecified)
#generate background covariates
X = mvtnorm::rmvnorm(n = N,
mean = rep(0,k),
sigma = diag(k)) #sample covariates -- what do you NOT notice here?
X
dim(X )
#Remember, G_i denotes the ever-treated status of i, NOT its treatment status
G_obs = rbinom(N, size = 1, prob = propensityTrue)
G_obs
propensityTrue = 1 / (1 + exp(- logitTrue ))
#GOAL 1. To illustrate the inverse propensity weighting method
#       by using Q6 in the homework.
#GOAL 2. Clarify the G_i, P_t, D_i notation using inverse propensity weighting
#In this study, we're going to study the setup in Q6 in the homework.
#In particular, we're going to check out how the bias + variance of the inverse-propensity weighting (IPW) estimator
#behaves under varying degrees of model misspecification!
#clear workspace
rm(list=ls())
library(glmnet) #load in glmnet package for lasso-penalized multinomial logistic reg.
set.seed(2)
T0 <- 1#1 pre-intervention period
T1 <- 1#1 post-intervention period
N = 1000 #total number of units in study
k = 20#how many covariates?
FracEverTreat = 0.5#fraction of units treated
tau = 0.25
seq_CovariateIncorrectlySpecified = round(c(0,k*0.25,k*0.5,k*0.75,k))
biasVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
varVec = rep(NA, times = length( seq_CovariateIncorrectlySpecified ))
counter_i <- 0
for(NumbCovarNonlinear in seq_CovariateIncorrectlySpecified){
print(sprintf("Getting results when %s of %s covariates incorrectly modeled",
NumbCovarNonlinear, k))
counter_i = counter_i + 1
#setup placeholder for inner results
#we need to average over many draws of the model
#with a fixed number of non-linear covariate relations
tauhat_vec = rep(NA,times=100)
for(i in 1:100){
#generate background covariates
X = mvtnorm::rmvnorm(n = N,
mean = rep(0,k),
sigma = diag(k)) #sample covariates -- what do you NOT notice here?
#these are the linear coefficients in the propensity model for G_i.
#we are going to manipulate the degree of non-linearity in X_i
#and see how our linear propensity score model (estimated via logistic regression)
#does at estimating the ATE.
propensity_coefs = rnorm(k)
if(NumbCovarNonlinear == 0){ logitTrue = scale(X %*% propensity_coefs + rnorm(N, sd = 0.25))}
if(NumbCovarNonlinear > 0){
X_transform = sapply(1:NumbCovarNonlinear,
function(i){
x = X[,i]
nonlinear_numb = sample(1:5,1)
if(nonlinear_numb == 1){x = sin(x)}
if(nonlinear_numb == 2){x = (x)^2}
if(nonlinear_numb == 3){x = log(abs(x))}
if(nonlinear_numb == 4){x = 1/(1+exp(-x))}
if(nonlinear_numb == 5){x = abs(x)}
return( x )
} )
X_tilde = cbind(X_transform,X[,-c(1:NumbCovarNonlinear)])
logitTrue = scale( X_tilde %*% propensity_coefs + rnorm(N, sd = 0.25) )
}
propensityTrue = 1 / (1 + exp(- logitTrue ))
#Remember, G_i denotes the ever-treated status of i, NOT its treatment status
G_obs = rbinom(N, size = 1, prob = propensityTrue)
#We could use G_i to construct D_{it}.
#Notice how G_i is indexed only by i. D_{it} is indexed by both i and t
PreIndicator = c(rep(0,times = T0),
rep(1,times = T1))
DMat = matrix(0,nrow = N,ncol=T0+T1)
DMat[G_obs==1,] <-t(replicate(c(PreIndicator), n = sum(G_obs==1)))
head(G_obs)
head(DMat)
#Notice the difference!
#Now, we can generate the observed outcomes.
outcome_coefs <- rnorm(k)
YPeriod0 <- X %*% outcome_coefs + rnorm(N, sd = 0.20)
YPeriod1 <- YPeriod0 + tau * G_obs + rnorm(N, sd = 0.50 * abs(YPeriod0)) #what is this scaling in the sd doing?
#Let's move on to the estimation!
#First, we need to estimate the propensity scores
#We  do so using lasso-regularized multionomial regression model
#regularization penalty chosen via 10-fold cross-validation
library(glmnet)
my_lasso = cv.glmnet(x = X,
y = as.matrix(G_obs),
family = "binomial",
type.measure = "class",
nfolds = 10,
alpha = 1)
#plot( my_lasso )
#Grab predicted probabilities
#s = lambda.min means that we want the predicted probabilities
#when the regularization penalty minimizes the mis-classification error
propensityEst = c( predict(my_lasso, s = "lambda.min", newx = X))
tauhat_vec[i] = mean( c(YPeriod1 - YPeriod0) * (G_obs - propensityEst) /
(propensityEst * (1-propensityEst)) )
}
#store bias and variance given DGP
biasVec[counter_i] <- mean(tauhat_vec - tau)
varVec[counter_i] <- var(tauhat_vec)
}
#Great! We're ready to plot our results
#Would you expect things to behave in a linear way or
#non-linear way with respect to the fraction of
#covariates incorrectly specified?
plot(seq_CovariateIncorrectlySpecified/k, abs(biasVec),
xlab = "fraction of covariates incorrectly specified",
ylab = "Absolute Bias")
#Great! We're ready to plot our results
#Would you expect things to behave in a linear way or
#non-linear way with respect to the fraction of
#covariates incorrectly specified?
plot(seq_CovariateIncorrectlySpecified/k, abs(biasVec),
xlab = "fraction of covariates incorrectly specified",
ylab = "Absolute Bias")
plot(seq_CovariateIncorrectlySpecified/k, varVec,
xlab = "fraction of covariates incorrectly specified",
ylab = "Variance")
foreign::read.dta
foreign::read.dta("./Downloads/Vietnam-2015-full data.dta")
my_dta = foreign::read.dta("./Downloads/Vietnam-2015-full data.dta")
my_dta = foreign::read.dta("./Downloads/Vietnam-2015-full data.dta")
head(my_dta[,1:5])
my_dta[1,]
mean(rewards_allP0_vec,na.rm=T)
mean(rewards_allP1_vec,na.rm=T)
mean(rewards_naive,na.rm=T)
rm(list=ls())
library(fuzzyjoin);library(tidyverse)
factor2numeric <- function(x){as.numeric(as.character(x))}
#setwd("~/Dropbox/Directory")
library(data.table)
devtools::install_github("cjerzak/LinkIt-software/LinkIt");
library(LinkIt)
head(LinkIt,20)
#LinkIt(x=c(), y=c(),by.x = "NM_LGL",by.y="comnam",parallelize = T)
LinkIt(x=c(), y=c(),by.x = "NM_LGL",by.y="comnam",parallelize = T)
load("./LinkIt_directory.Rdata")
load
head(load)
load("./LinkIt_directory_trigrams.Rdata")
load("./LinkIt_directory.Rdata")
load("./LinkIt_directory.Rdata")
data("LinkIt_directory_trigrams.Rdata", "LinkIt_directory_trigrams.Rdata", package="LinkIt", envir=parent.env(environment()))
devtools::install_github(sprintf("cjerzak/%s-software/%s",package_name,package_name))
.onLoad
devtools::install_github(sprintf("cjerzak/%s-software/%s",package_name,package_name))
##################################################
##INSTRUCTIONS FOR PACKAGE DOWNLOAD############
##################################################
package_name <- "LinkIt"
#Generate documentation
{
setwd(sprintf("~/Dropbox/Directory/%s-software",package_name))
devtools::document(sprintf("./%s",package_name))
try(file.remove(sprintf("./%s.pdf",package_name)),T); system(sprintf("R CMD Rd2pdf %s",package_name))
}
#Install package
devtools::install_github(sprintf("cjerzak/%s-software/%s",package_name,package_name))
q()
